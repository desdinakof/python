# -*- coding: utf-8 -*-
"""prachReceiver.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T2kJSb2qYFjs1Gf5nYGLp1RaJEGrx6fJ
"""
"""**Importing Libraries**"""

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from numpy import loadtxt
import tensorflow as tf
import tensorflow.keras.models as km
import tensorflow.keras.layers as kla
import tensorflow.keras.optimizers as ko
import tensorflow.keras.losses as klo
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score,r2_score
import time
import statistics as st

"""**Downloading Datasets**"""
def PrincipalCA(dataset, number_components):
    #for i in range(len(x))
    from sklearn.decomposition import PCA
    pca = PCA(n_components=number_components)
    principalComponents = pca.fit_transform(dataset)
    principalDf = pd.DataFrame(data = principalComponents
             ,columns = ['principal component {}'.format(i) for i in range(number_components)])
  
    return principalDf

def Nmaxelements(list_to_iterate, previous_list, iteration_number):
    if previous_list == None:
       previous_list = []
  
    for i in range(0, iteration_number): 
        max1 = 0
        for j in range(len(list_to_iterate)):     
            if list_to_iterate[j] > max1:
                max1 = list_to_iterate[j];
                  
        previous_list.append(max1)
        previous_list.append(list_to_iterate.index(max1))
 #       list_to_iterate.remove(max1);
    return previous_list

def preprocess_data_from_csv(csv_path):
    df = pd.read_csv(csv_path)
    #df = scipy.io.loadmat("/content/drive/My Drive/Colab Notebooks/datasets/AwgnTx1Rx2Scs15A1PrIx00Cf01It01bd.mat")
    #df5 = pd.read_csv("/content/drive/My Drive/Colab Notebooks/datasets/AwgnTx1Rx2Scs15A1PrIx00Cf01It01.csv", usecols = ['PrachActive', '0'],dtype = np.float64)

    """**Labeling**"""

    df.columns = ['Position', 'Label','Correlation']
    df['Label'] = df['Label'].map({"PrachRxCorr": 0, "PrachActive": 1}).astype(float)

    d = {'Position': ['0 0 2'], 'Label': 1.0, 'Correlation': '0'}
    df2 = pd.DataFrame(data=d)

    mat = df2.append(df, ignore_index= True)
    del df2, df, d

    """**Converting String to List and Combining The Rows**"""

    mat_copy = mat.copy()
    mat_copy.iloc[:,2] = mat_copy.iloc[:,2].apply(lambda x: [float(i) for i in x.replace(";", " ").split(" ") if i != ""])
    # APPLY değil de TRANSFORMla yapmayı dene
    mat = mat_copy

    mat_cp = mat.copy(deep = True)
    rows = []
    for i in mat_cp.index:
      row = mat_cp.iloc[i]
      if row.Correlation == [1.0] or row.Correlation == [0.0]:
        mat_copy3 = []
        j = i+1
        next_row = mat_cp.iloc[j]
        while next_row.Correlation != [1.0] and next_row.Correlation != [0.0] and j< len(mat_cp.index):
          mat_copy3 += next_row.Correlation
          next_row = mat_cp.iloc[j]
          j +=1
        row.Label = row.Correlation
        row.Correlation = mat_copy3
        rows.append(row)
        
    del mat_cp, mat_copy, row
    matter = pd.DataFrame(rows)
    del rows
    print("1")

    #mat_cp2 = matter.copy(deep = True)

    #import statistics as st

    #mat_cp2.iloc[:,2] = mat_cp2.iloc[:,2].apply(lambda x: Nmaxelements(x, [x.index(max(x)), max(x)], 3) if x else x )
    #matter = mat_cp2
    #print(matter)

    #data separation to train and test
    #x = np.array(matter.iloc[:,2], dtype=np.float)
    #y = np.array(matter.iloc[:,1], dtype=np.float)
    x = matter.iloc[:,2]
    y = matter.iloc[:,1]
    
    del matter
    
    x = pd.DataFrame(x.tolist()[:-1])
    y = y.apply(lambda i: float(i[0])).iloc[:-1]

    print("2")
    #Encoding for Target Values

    label_encoder = LabelEncoder()
    y2 = label_encoder.fit_transform(y)
    
    y3 = pd.DataFrame(y2.tolist())
    del y2
    #Scaling for X values
    scaler = StandardScaler()
    df2 = scaler.fit_transform(x)

    df3 = pd.DataFrame(df2.tolist())
    del df2
    #df3.loc('None')
    print("3")
    index = df3[df3.isnull().any(axis=1)].index
    if (len(index) > 0):
      df3.drop(inplace=True, index=index)
      y3.drop(inplace=True, index=index)

    return df3, y3


x, y = preprocess_data_from_csv("D:/AwgnTx1Rx2Scs15A1PrIx00Cf01It01_v2.csv")
print("4")
x2 = PrincipalCA(x, 10)
print("5")
x_train, x_test, y_train, y_test = train_test_split(x2, y, train_size=0.75, test_size=0.25, shuffle=True)
print("6")

#x_train.dropna(axis=0, inplace=True)
#y_train.dropna(axis=0, inplace=True)

#print(x_train, "\n\n", y_train, "\n\n", x_test, "\n\n", y_test)

# ANN
#opt = ko.SGD(learning_rate=0.01)

model = km.Sequential()
model.add(kla.Dense(128, input_dim=10, activation='relu'))
model.add(kla.Dropout(0.5))
model.add(kla.Dense(32, activation='relu'))
model.add(kla.Dense(16, activation='relu'))
#model.add(kla.Dense(16, activation='relu'))
model.add(kla.Dense(1, activation='sigmoid'))

#bce = klo.BinaryCrossentropy(from_logits = True, label_smoothing = 0, axis= -1, reduction = "auto")
#model.compile(loss= 'poisson', optimizer= 'adam', metrics=['accuracy'])
model.compile(optimizer="adam",loss=klo.BinaryCrossentropy(),
              metrics=tf.keras.metrics.BinaryAccuracy())
              #,tf.keras.metrics.FalseNegatives()])
# fit the keras model on the dataset
model.fit(x_train, y_train, batch_size= 32, epochs= 10)

# evaluate the keras model
_, accuracy_mdl = model.evaluate(x_train, y_train)

# extract the predicted probabilities
p_pred = model.predict(x_test)
p_pred = p_pred.flatten()
print(p_pred.round(2))

# extract the predicted class labels
y_hat_test = np.where(p_pred > 0.5, 1, 0)
print(y_hat_test)

print("Model Result:")
print(confusion_matrix(y_test, y_hat_test))
print(classification_report(y_test, y_hat_test))

#loss accuracy plot

for i in range(2,7):
    print("Running file index: {:02d} for Model Generalization".format(i))
    x, y = preprocess_data_from_csv("D:/AwgnTx1Rx2Scs15A1PrIx00Cf01It{:02d}_v2.csv".format(i))
    print("8")
    x2 = PrincipalCA(x, 10)
    print("9")
    x_train, x_test, y_train, y_test = train_test_split(x2, y, train_size=0.75, test_size=0.25, shuffle=True)
    
    del x,y, x2
    print("10")
    """
    model.compile(optimizer="adam",loss=klo.BinaryCrossentropy(),
                  metrics=tf.keras.metrics.BinaryAccuracy())
    score = model.evaluate(x_train, y_train, verbose=0)
    print("%s: %.2f%%" % (model.metrics_names[1], score[1]*100))
    del x_train, y_train, x_test, y_test
    """
    
    p_pred = model.predict(x_test)
    p_pred = p_pred.flatten()
    print(p_pred.round(2))
    
    # extract the predicted class labels
    y_hat_test = np.where(p_pred > 0.5, 1, 0)
    print("Predicted y values are {}".format(y_hat_test))
   
    print("Confusion matrix is")
    print(confusion_matrix(y_test, y_hat_test))
    print(classification_report(y_test, y_hat_test))
    del p_pred, y_test, y_hat_test